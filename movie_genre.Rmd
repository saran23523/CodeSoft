---
title: "movie_genre"
author: "Saran Babu A"
date: "2024-06-30"
output: html_document
---



Importing necessary packages
```{python}
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re
import nltk
import string
from nltk.stem import LancasterStemmer  
from nltk.corpus import stopwords

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split, RandomizedSearchCV, KFold
from sklearn.metrics import accuracy_score, classification_report
from sklearn.svm import LinearSVC
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier  # Explore ensemble methods

```


Loading the training data
```{python}

train_path = "C:/Users/Saran Babu/Downloads/archive (3)/Genre Classification Dataset/train_data.txt"
train_data = pd.read_csv(train_path,sep=":::",names=['Movieid','Title','Genre','Description'],engine='python')

print(train_data.Title)

```


Loading the test data
```{python}
test_path = "C:/Users/Saran Babu/Downloads/archive (3)/Genre Classification Dataset/test_data.txt"
test_data = pd.read_csv(test_path,sep=":::",names=['Movieid','Title','Description'],engine='python')
print(test_data.Title)
```


EDA and graphoical visualization  --- skipped ...
```{python}
plt.figure(figsize=(14,7))

```



Data preprocessing and text cleaning
```{python}
stemmer = LancasterStemmer()
#nltk.download('stopwords')
#nltk.download('punkt')
stop_words = set(stopwords.words('english'))




def clean(text):
  text = text.lower()
  text = re.sub(r'http\S+','',text)
  text = re.sub(r"[^a-zA-Z+']", ' ', text)
  text = re.sub(r"[^a-zA-Z+']", ' ', text)
  text = re.sub(r'\s+[a-zA-Z]\s+', ' ', text + ' ')
  text = "".join([i for i in text if i not in string.punctuation])
  stopwords = nltk.corpus.stopwords.words('english')
  words = nltk.word_tokenize(text)
  text = " ".join([i for i in words if i not in stopwords and len(i) > 2])
  text = re.sub("\s[\s]+", " ", text).strip()

  return text

train_data['Text_cleaning'] = train_data['Description'].apply(clean)
test_data['Text_cleaning'] = test_data['Description'].apply(clean)
print(train_data['Text_cleaning'][1])
```



Preprocessed data visualization :

```{python}

train_data['cleaned_text_len'] = train_data['Text_cleaning'].apply(len)
plt.figure(figsize=(8, 7))
sns.histplot(data = train_data, x = 'cleaned_text_len', bins = 20 , kde = True , color='red')
plt.xlabel('Length')
plt.ylabel('Frequency')
plt.title('Distribution of lengths')
plt.show()

```


Vectorization into TF-IDF matrix : 

```{python}
tf_idf = TfidfVectorizer(ngram_range=(1, 2), min_df=2)
X_traindata = tf_idf.fit_transform(train_data['Text_cleaning'])
X_test = tf_idf.transform(test_data['Text_cleaning'])

print(X_traindata.shape)
```



Training and prediction : 

```{python}
X = X_traindata
y = train_data['Genre']
X_train , X_val, Y_train, Y_val = train_test_split(X , y , test_size=0.2 , random_state=42) 


#Hyper parameters tuning
param_grid = {
    'C': [0.1, 1, 10, 100],
    'loss': ['hinge', 'squared_hinge'],
    'class_weight': [None, 'balanced']
}


model = LinearSVC()
model.fit(X_train,Y_train)
Y_pred = model.predict(X_val)



random_search = RandomizedSearchCV(model, param_grid, cv=5, scoring='accuracy', n_iter=3, random_state=42)
random_search.fit(X_train,Y_train)

best_model = random_search.best_estimator_
best_params = random_search.best_params_

Y_pred = best_model.predict(X_val)

accuracy = accuracy_score(Y_val, Y_pred)
print(accuracy)

```



```{python}

X_test_predict = best_model.predict(X_test)
test_data['Predicted_Genre'] = X_test_predict

test_data.to_csv('result.csv', index=False)
print(test_data)


```